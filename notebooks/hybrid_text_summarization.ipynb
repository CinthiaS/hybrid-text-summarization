{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"hybrid_text_summarization.ipynb","private_outputs":true,"provenance":[{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb","timestamp":1583973910794}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"J0Qjg6vuaHNt"},"source":["# Abstractive Text Summarization using Attention and Beam Search"]},{"cell_type":"code","metadata":{"id":"dUXmhjKeOPuC"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZHKJU9bYVa3T"},"source":[""]},{"cell_type":"code","metadata":{"id":"tnxXKDjq3jEL"},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","!pip install nltk\n","\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import pandas as pd\n","import os\n","import io\n","import time\n","import re\n","import requests\n","import json\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import os\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yPHgnvjiObfh"},"source":["## Preprocess"]},{"cell_type":"code","metadata":{"id":"5dby5VI6OUjO"},"source":["def load_files(\n","    train_article_path, train_title_path, valid_article_path, valid_title_path):\n","    \"\"\"\n","    Lê os arquivos de entrada.\n","\n","    Parameters\n","    ----------\n","    train_article_path : str\n","        Caminho do arquivo que possui os artigos da base de dados de treinamento.\n","    train_title_path : str\n","        Caminho do arquivo que possui os resumos da base de dados de treinamento.\n","    valid_article_path : str\n","        Caminho do arquivo que possui os artigos da base de dados de validacao.\n","    valid_title_path : str\n","        Caminho do arquivo que possui os titulos da base de dados de validacao.\n","    Returns\n","    -------\n","    article_train : artigos da base de treino.\n","    summary_train : titulos da base de treino.\n","    article_valid : artigos da base de validacao.\n","    summary_valid : titulos da base de validacao. \n","    \"\"\"\n","\n","    article_train = open(train_article_path,'r') \n","    summary_train = open(train_title_path,'r') \n","    article_valid = open(valid_article_path,'r')\n","    summary_valid = open(valid_title_path ,'r') \n","\n","    return article_train, summary_train, article_valid, summary_valid\n","\n","def remove_stopwords(line):\n","    \"\"\"\n","    Remove stopwords.\n","\n","    Parameters\n","    ----------\n","    line : str\n","        Sentença com 'n' palavras.\n","    Returns\n","    -------\n","    sentence : sentença com 'n' palavras sem stopwords.\n","    \"\"\"\n","\n","    stop_words = set(stopwords.words('english'))\n","    tokens = word_tokenize(line)\n","    sentence = [i for i in tokens if not i in stop_words]\n","    \n","    return sentence\n","\n","def preprocess(file):\n","    \"\"\"\n","    Remove caracteres especias e ruídos do texto.\n","\n","    Parameters\n","    ----------\n","    file : object\n","        Object de arquivo.\n","    Returns\n","    -------\n","    processed : arquivo pré-processado sem caracteres especiais ou ruídos.\n","    \"\"\"\n","    \n","    processed = [] # preprocess the file\n","    lines = file.read().split('\\n')\n","    for line in lines:\n","        line = line.lower() # lower case\n","        line = line.replace('#','') # replace the token '#' with '<num>'\n","        line = line.replace(',','')\n","        line = line.replace('-','')\n","        line = line.replace('-','')\n","        #Remove strings compostas por caracteres unicos\n","        line = re.sub(r'\\b\\w\\b', '', line)\n","        line = re.sub(r'\\d+', '', line)\n","        text = re.sub(r\"[()`#/@';:%<>$&\\\"{}~+=?|]\", \" \", line) # replace other tokens with a space\n","        text = text.rstrip().strip() # strip white space\n","        text = text.replace(\".\",\"#\")\n","        processed.append(text)\n","    del(processed[len(processed)-1])\n","\n","    return processed\n","\n","def addTokens(\n","    lines, type_input, summary_max_len, aricle_max_len, stopwords=False): \n","    \"\"\"\n","    Filtra o texto de entrada de acordo com o limite de tamanho pré-estabelecido\n","    e adiciona os caracteres de inicio (sostok) e fim (eostok) de sentença. Somente\n","    os texto do tipo article podem ter suas stopwords removidas.\n","\n","    Parameters\n","    ----------\n","    lines : str\n","        Sentença com 'n' palavras.\n","    type_input : str\n","        Tipo de entrada, pode ser summary or article.\n","    summary_max_len : int\n","        Tamanho máximo do resumo de saída.\n","    aricle_max_len : int\n","        Tamanho máximo do texto de entrada.\n","    stopwords : bool\n","        If true, remove stopwords.\n","    Returns\n","    -------\n","    sentence : sentença com 'n' palavras sem stopwords.\n","    \"\"\"\n","\n","    textos = []\n","    for line in lines:\n","        if(type_input == \"summary\"):\n","          text = line.split(\" \")[0:summary_max_len]\n","          text = \" \".join(text)\n","          text = 'sostok ' + text + ' eostok' # beginning and end tokens for each sentence\n","        elif(type_input == \"article\"):\n","          if(stopwords):\n","            text = remove_stopwords(line)\n","            text = text[0:article_max_len]\n","          else:\n","            text = line.split(\" \")[0:article_max_len]\n","          text = \" \".join(text)\n","          text = 'sostok ' + text + ' eostok'\n","        textos.append(text)\n","\n","    return textos"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o_C7ZqdqOo6-"},"source":["## Tokenizer"]},{"cell_type":"code","metadata":{"id":"eFnyC15VOuWN"},"source":["def tokenizer_texts(\n","    mode, name_model, all_text, article_max_len, summary_max_len,\n","    article_train, summary_train, path_save, verbose=False):\n","    \"\"\"\n","    Cria tokenizador pro texto. Se o caminho do diretório do modelo já existir ou\n","    a função for chamada em modo de validação, apenas carrega um tokenizador já existente\n","    caso contrário cria tokenizador.\n","\n","    Parameters\n","    ----------\n","    mode : str\n","        Indica se está executando treinamento ou validação.\n","    name_model : str\n","        Nome do modelo.\n","    all_text : \n","        Todos os textos de entrada.\n","    article_max_len : int\n","        Tamanho máximo do texto de entrada.\n","    summary_max_len : int\n","        Tamanho máximo do texto de saída.\n","    article_train : list\n","        Lista com todos os artigos.\n","    summary_train : lis\n","        Lista com todos os resumos.\n","    verbose : bool\n","        If true, utiliza os prints.\n","    Returns\n","    -------\n","    vocab_size : tamanho do vocabulário criado, o vocabulário é crido apenas com as palavras da base de treino.\n","    tokenizer : objeto tokenizer.\n","    article_train : artigo após aplicação do padding.\n","    summary_train : resumo após aplicação do padding.\n","    \"\"\"\n","\n","    if not os.listdir(path_save + name_model):\n","\n","      if verbose:\n","        print(\"Create a new Tokenizer\")\n","\n","      tokenizer = Tokenizer() \n","      tokenizer.fit_on_texts(list(all_text))\n"," \n","      with open(path_save + name_model + '/tokenizer.pickle', 'wb') as handle:\n","        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    elif(os.listdir(path_save + name_model) or mode == \"VALID\"):\n","\n","      if(verbose):\n","        print(\"Load a old Tokenizer\")\n","\n","      with open(path_save + name_model + '/tokenizer.pickle', 'rb') as handle:\n","        tokenizer = pickle.load(handle)\n","\n","    x_tr_seq    =   tokenizer.texts_to_sequences(article_train) \n","    article_train    =   pad_sequences(x_tr_seq,  maxlen=article_max_len, padding='post')\n","\n","    y_tr_seq    =   tokenizer.texts_to_sequences(summary_train) \n","    summary_train    =   pad_sequences(y_tr_seq, maxlen=summary_max_len, padding='post') \n","    vocab_size = len(tokenizer.word_index)+1 \n","\n","    return vocab_size, tokenizer, article_train, summary_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_5qjoGB2PDPo"},"source":["## Data Preparation"]},{"cell_type":"code","metadata":{"id":"JNkqdlESO1Hj"},"source":["def data_preparation(\n","    mode, name_model, train_article_path, train_title_path,\n","    valid_article_path, valid_title_path, path_save, article_max_len=150,\n","    summary_max_len=15, verbose=False):\n","  \n","    \"\"\"\n","    Pipeline para realizar todos as etapas de preparação dos dados para entrada do\n","    modelo.\n","\n","      1. Load files\n","      2. Preprocess text\n","      3. Add start an end tokens\n","      4. Save inputs\n","      5. Remove header\n","      6. Tokenizer\n","      7. Create output\n","\n","    Parameters\n","    ----------\n","    mode : str\n","        Tipo de execução, treino ou validação.\n","    name_model : str\n","        Nome do modelo.\n","    train_article_path : str\n","        Caminho do arquivo que possui os artigos da base de dados de treinamento.\n","    train_title_path : \n","        Caminho do arquivo que possui os titulos da base de dados de treinamento.\n","    valid_article_path : \n","        Caminho do arquivo que possui os artigos da base de dados de validacao.\n","    valid_title_path : \n","        Caminho do arquivo que possui os titulos da base de dados de validação.\n","    article_max_len : int,  default = 150\n","        Tamanho maximo do artigo de entrada.\n","    summary_max_len : int , default = 15\n","        Tamanho máximo do resumo de saida.\n","    verbose : bool\n","        If true, utiliza os prints.\n","    Returns\n","    -------\n","    dataset : base de treino formatada.\n","    steps_per_epoch : quantidade de etapas em cada epoca de treinamento.\n","    tokenizer : objeto tokenizer.\n","    vocab_size : tamanho do vocabulário.\n","    \"\"\"\n","\n","   #1. Load Files\n","    article_train,summary_train,article_valid,summary_valid = load_files(train_article_path, train_title_path,\n","                                                                     valid_article_path, valid_title_path)\n","\n","    # 2. Preprocess\n","    lines_article_train = preprocess(article_train)\n","    lines_summary_train = preprocess(summary_train)\n","    lines_article_valid = preprocess(article_valid)\n","    lines_summary_valid = preprocess(summary_valid)\n","\n","    # 3. Add end and start tokens\n","    lines_article_train = addTokens(lines_article_train, \"article\",summary_max_len, article_max_len, False)  \n","    lines_summary_train = addTokens(lines_summary_train, \"summary\",summary_max_len, article_max_len, False)\n","    lines_article_valid = addTokens(lines_article_valid, \"article\",summary_max_len, article_max_len, False)\n","    lines_summary_valid = addTokens(lines_summary_valid, \"summary\",summary_max_len, article_max_len, False)\n","\n","    all_text = lines_article_train + lines_summary_train + ['unktok']\n","\n","    # 4. Save\n","    TRAIN = pd.DataFrame({'Input':lines_article_train,'Summary':lines_summary_train})\n","    VALID = pd.DataFrame({'Input':lines_article_valid,'Summary':lines_summary_valid})\n","\n","    # 5. Remove header\n","    article_train = TRAIN.iloc[:,0].values\n","    summary_train = TRAIN.iloc[:,1].values\n","    article_valid = VALID.iloc[:,0].values\n","    summary_valid = VALID.iloc[:,1].values\n","\n","    # 6. Tokenizer\n","    vocab_size,tokenizer,article_train_token,summary_train_token = tokenizer_texts(mode, name_model, all_text, article_max_len,\n","                                                                               summary_max_len, article_train, summary_train,\n","                                                                               path_save=path_save, verbose=verbose)\n","    BUFFER_SIZE = len(article_train)\n","    steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n","\n","    # 7. Create output\n","    dataset = tf.data.Dataset.from_tensor_slices((article_train_token, summary_train_token)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","    if(verbose):\n","      print(\"Maximum input size: \" + str(article_max_len))\n","      print(\"Maximum output abstract size: \" + str(summary_max_len))\n","      print(\"Vocabulary size: \" + str(vocab_size) + '\\n')\n","\n","    return dataset, VALID, steps_per_epoch, tokenizer, vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CplvZlxVPkc9"},"source":["## Load Word Embeddings"]},{"cell_type":"code","metadata":{"id":"bON8D8OgPo92"},"source":["def create_embeddings(\n","    tokenizer, article_max_len, vocab_size, path_save, number_model, verbose=False):\n","    \"\"\"\n","    Create a embedding matrix se esse não existir, caso exista apenas carrega o\n","    arquivo existente.\n","\n","    Parameters\n","    ----------\n","    tokenizer : object\n","        Objeto tokenizer.\n","    article_max_len : int\n","        Tamanho máximo do texto de entrada.\n","    vocab_size : int\n","        Tamanho do vocabulario.\n","    path : str\n","        Caminho base.\n","    verbose : bool\n","        If true, usa os prints\n","    Returns\n","    -------\n","    embedding_matrix\n","    \"\"\"\n","\n","    MAX_SEQUENCE_LENGTH=article_max_len\n","    MAX_NUM_WORDS = vocab_size + 2\n","    num_words = vocab_size\n","\n","    if verbose:\n","      print('Indexing word vectors.')\n","\n","    if not os.path.isfile(path_save + number_model + '/embedding_matrix.pickle'):\n","\n","        if verbose:\n","          print(\"Create a new embedding matrix\")\n","\n","        embeddings_index = {}\n","        with open('/content/drive/My Drive/Colab Notebooks/glove/glove.6B.300d.txt') as f:\n","          for line in f:\n","            word, coefs = line.split(maxsplit=1)\n","            coefs = np.fromstring(coefs, 'f', sep=' ')\n","            embeddings_index[word] = coefs\n","\n","        if verbose:\n","          print('Found %s word vectors.' % len(embeddings_index))\n","\n","        word_index = tokenizer.word_index\n","\n","        if verbose:\n","          print('Found %s unique tokens.' % len(word_index))\n","\n","        # prepare embedding matrix\n","        num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n","        embedding_matrix = np.zeros((num_words, embedding_dims))\n","        for word, i in word_index.items():\n","          if i >= MAX_NUM_WORDS:\n","            continue\n","          embedding_vector = embeddings_index.get(word)\n","          if embedding_vector is not None:\n","              # words not found in embedding index will be all-zeros.\n","              embedding_matrix[i] = embedding_vector\n","\n","        with open(path_save + number_model + '/embedding_matrix.pickle', 'wb') as handle:\n","          pickle.dump( embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    \n","    else:\n","\n","        if verbose:\n","          print(\"Load embedding matrix\")\n","\n","        with open(path_save + number_model + '/embedding_matrix.pickle', 'rb') as handle:\n","          embedding_matrix = pickle.load(handle)\n","\n","    return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rgCLkfv5uO3d"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"nZ2rI24i3jFg"},"source":["from tensorflow.keras.layers import  Bidirectional, LSTM, Concatenate\n","\n","class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_matrix, embedding_dims=300, dropout=0.8, hidden_size=150, batch_size=64, name_layer='BiLSTM'):\n","    super(Encoder, self).__init__()\n","    \"\"\"\n","    Codificador do modelo. \n","\n","    Parameters\n","    ----------\n","    vocab_size : int\n","        Tamanho do vocabulário.\n","    embedding_matrix : list\n","        Matrix de embeddings.\n","    embedding_dims : int, default 300\n","        Dimensão do vetor de embeddings.\n","    dropout : float\n","        Quantidade de neurônios mantidos.\n","    hidden_size : int, default 150\n","        Quantidade de neuronios da camada oculta.\n","    batch_size : int, default 64\n","        Quantidade de batch.\n","    name_layer : str, default BiLSTM\n","        Tipo de rede utilizada, LSTM, GRU, BiGRU ou BiLSTM\n","    Returns\n","    -------\n","    processed : arquivo pré-processado sem caracteres especiais ou ruídos.\n","    \"\"\"\n","\n","    self.vocab_size=vocab_size\n","    self.batch_size=batch_size\n","    self.name_layer=name_layer\n","    self.dropout=dropout\n","    self.hidden_size=hidden_size\n","    self.embedding_matrix=embedding_matrix\n","\n","    self.encoder_embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size, \n","                                                           weights=[self.embedding_matrix],\n","                                                           output_dim=embedding_dims,\n","                                                           trainable=False)\n","    if(self.name_layer == \"GRU\"):\n","      self.gru1 = tf.keras.layers.GRU(self.hidden_size,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","      self.gru2 = tf.keras.layers.GRU(self.hidden_size,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    elif(self.name_layer == \"LSTM\"):\n","      self.lstm1 = tf.keras.layers.LSTM(self.hidden_size,\n","                                   return_sequences=True,\n","                                   return_state=True)\n","      \n","      self.lstm2 = tf.keras.layers.LSTM(self.hidden_size,\n","                                   return_sequences=True,\n","                                   return_state=True)\n","    elif(self.name_layer == \"BiLSTM\"):\n","      self.BiLSTM1 = Bidirectional(LSTM(self.hidden_size,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       dropout=self.dropout))\n","      self.BiLSTM2 = Bidirectional(LSTM(self.hidden_size,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       dropout=self.dropout))\n","    elif(self.name_layer == \"BiGRU\"):\n","      self.BiGRU1 = Bidirectional(tf.keras.layers.GRU(self.hidden_size,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       dropout=self.dropout))\n","      self.BiGRU2 = Bidirectional(tf.keras.layers.GRU(self.hidden_size,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       dropout=self.dropout))\n","\n","  def call(self, x, hidden):\n","\n","    x = self.encoder_embedding(x)\n","\n","    if(self.name_layer == \"GRU\"):\n","      encoder_outputs_1, *_ = self.gru1 (x, initial_state=hidden)\n","      encoder_outputs, encoder_states = self.gru2 (encoder_outputs_1)\n","    \n","    elif(self.name_layer == \"LSTM\"):\n","      encoder_outputs_1, *_ = self.lstm1 (x, initial_state = hidden)\n","      encoder_outputs, encoder_states, *_ = self.lstm2 (encoder_outputs_1)\n","    \n","    elif(self.name_layer == \"BiLSTM\"):\n","      encoder_outputs_1, *_ = self.BiLSTM1 (x,initial_state = hidden)\n","      encoder_outputs, forward_h, forward_c, backward_h, backward_c, *_ = self.BiLSTM2 (encoder_outputs_1)\n","      state_h = Concatenate()([forward_h, backward_h])\n","      state_c = Concatenate()([forward_c, backward_c])\n","      encoder_states = Concatenate()([state_h, state_c])\n","    \n","    elif(self.name_layer == \"BiGRU\"):\n","      encoder_outputs_1, *_ = self.BiGRU1 (x,initial_state = hidden)\n","      encoder_outputs, forward_h, forward_c, backward_h, backward_c, *_ = self.BiGRU2 (encoder_outputs_1)\n","      state_h = Concatenate()([forward_h, backward_h])\n","      state_c = Concatenate()([forward_c, backward_c])\n","      encoder_states = Concatenate()([state_h, state_c])\n","\n","    return encoder_outputs, encoder_states\n","\n","  def initialize_hidden_state(self):\n","      if(self.name_layer == \"LSTM\" or self.name_layer == \"GRU\"):\n","        return (tf.zeros([self.batch_size, self.hidden_size]),\n","              tf.zeros([self.batch_size, self.hidden_size]))\n","      elif (self.name_layer == \"BiLSTM\" or self.name_layer == \"BiGRU\"):\n","        return (tf.zeros([self.batch_size, self.hidden_size]),\n","              tf.zeros([self.batch_size, self.hidden_size]),\n","              tf.zeros([self.batch_size, self.hidden_size]),\n","              tf.zeros([self.batch_size, self.hidden_size]))\n","        \n","class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    # query hidden state shape == (batch_size, hidden size)\n","    # query_with_time_axis shape == (batch_size, 1, hidden size)\n","    # values shape == (batch_size, max_len, hidden size)\n","    # we are doing this to broadcast addition along the time axis to calculate the score\n","\n","    query_with_time_axis = tf.expand_dims(query, 1)\n","\n","    #print(values)\n","    # score shape == (batch_size, max_length, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    score = self.V(tf.nn.tanh(\n","        self.W1(query_with_time_axis) + self.W2(values)))\n","    \n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights\n","\n","class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_matrix, embedding_dim, hidden_size, batch_size, name_layer, name_layer_encoder):\n","    super(Decoder, self).__init__()\n","    \n","    self.vocab_size=vocab_size\n","    self.batch_size = batch_size\n","    self.name_layer = name_layer\n","    self.embedding_matrix = embedding_matrix\n","\n","    if(name_layer_encoder == \"LSTM\" or name_layer_encoder == \"GRU\"): self.hidden_size = hidden_size\n","    elif(name_layer_encoder == \"BiLSTM\" or name_layer_encoder == \"BiGRU\"): self.hidden_size = hidden_size*4\n","\n","    self.decoder_embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size, \n","                                                           weights=[self.embedding_matrix],\n","                                                           output_dim=embedding_dims,\n","                                                           trainable=False)\n","    \n","    if(self.name_layer == \"GRU\"):\n","      self.gru = tf.keras.layers.GRU(self.hidden_size,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    elif(self.name_layer == \"LSTM\"):\n","      self.lstm = tf.keras.layers.LSTM(self.hidden_size,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","      \n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","    # used for attention\n","    self.attention = BahdanauAttention(self.hidden_size)\n","    #self.attention = AttentionLayer(self.hidden_size)\n","\n","  def call(self, x, hidden, enc_output):\n","    #print(enc_output)\n","    # enc_output shape == (batch_size, max_length, hidden_size)\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.decoder_embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the GRU\n","    if(self.name_layer == \"GRU\"):\n","      #GRU Layer 1\n","      decoder_outputs, decoder_states = self.gru (x)\n","    \n","    elif(self.name_layer == \"LSTM\"):\n","      #LSTM Layer 1\n","      decoder_outputs, decoder_states, *_ = self.lstm (x)\n","\n","    # output shape == (batch_size * 1, hidden_size)\n","    decoder_outputs = tf.reshape(decoder_outputs, (-1, decoder_outputs.shape[2]))\n","\n","\n","    # output shape == (batch_size, vocab)\n","    x = self.fc(decoder_outputs)\n","\n","    return x, decoder_states, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ch_71VbIRfK"},"source":["## Define the optimizer and the loss function"]},{"cell_type":"code","metadata":{"id":"WmTHr5iV3jFr"},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)\n","\n","@tf.function\n","def train_step(inp, targ, enc_hidden, encoder, decoder):\n","  loss = 0\n","\n","  with tf.GradientTape() as tape:\n","    \n","    enc_output, enc_hidden = encoder(inp, enc_hidden)\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([tokenizer.word_index['sostok']] * BATCH_SIZE, 1)\n","\n","    for t in range(1, targ.shape[1]):\n","      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","      loss += loss_function(targ[:, t], predictions)\n","      dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","  batch_loss = (loss / int(targ.shape[1]))\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","  gradients = tape.gradient(loss, variables)\n","  optimizer.apply_gradients(zip(gradients, variables))\n","\n","  return batch_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hpObfY22IddU"},"source":["## Training\n"]},{"cell_type":"code","metadata":{"id":"ddefjBMa3jF0"},"source":["import time\n","import datetime\n","\n","\n","def model_training(vocab_size, embedding_matrix, embedding_dims, hidden_size, BATCH_SIZE, name_layer_encoder, name_layer_decoder, path_save, steps_per_epoch, dropout):\n","\n","  #Create model e Load checkpoint\n","  encoder = Encoder(vocab_size, embedding_matrix, embedding_dims=embedding_dims, dropout=dropout, hidden_size=hidden_size, batch_size=BATCH_SIZE, name_layer=name_layer_encoder)\n","  decoder = Decoder(vocab_size, embedding_matrix, embedding_dims, hidden_size, BATCH_SIZE, name_layer_decoder, name_layer_encoder)\n","  \n","  checkpoint_dir = path_save + number_model + '/'\n","  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","  checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                  encoder=encoder,\n","                                  decoder=decoder)\n","  manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)\n","\n","  if manager.latest_checkpoint:\n","      print(\"Restored from {}\".format(manager.latest_checkpoint))\n","      checkpoint.restore(manager.latest_checkpoint)\n","      EPOCHS = 100-(int(manager.latest_checkpoint.split('/')[-1].split('-')[-1]))\n","      passo = int(manager.latest_checkpoint.split('/')[-1].split('-')[-1]) +1\n","  else:\n","      print(\"Initializing\")\n","      EPOCHS = 100\n","      passo = 0\n","\n","  print(\"\\nStarting Model Training\\n\")\n","  print(\"Number of epochs: \" + str(EPOCHS))\n","  print(\"Batch size: \" + str(BATCH_SIZE) + \"\\n\\n\")\n","\n","  with open(path_save + number_model + '/time_hybrid.txt', 'a') as f:\n","    f.write('Model: {} Inicio: {}'.format(number_model, datetime.datetime.now()))\n","\n","  start = time.time()\n","  for epoch in range(EPOCHS):\n","\n","    enc_hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","\n","    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","      batch_loss = train_step(inp, targ, enc_hidden, encoder, decoder)\n","      total_loss += batch_loss\n","      \n","      if batch % 400 == 0:\n","        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + passo,\n","                                                    batch,\n","                                                    batch_loss.numpy()))\n","    # saving (checkpoint) the model every 2 epochs\n","    if (epoch + 1) % 1 == 0:\n","      manager.save()\n","\n","    print('Epoch {} Loss {:.4f}'.format(epoch + passo,\n","                                        total_loss / steps_per_epoch))\n","    with open(path_save + number_model + '/history.txt','a') as f:\n","      f.write('Epoch {} Loss {:.4f}'.format(epoch +passo,\n","                                        total_loss / steps_per_epoch))\n","    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","\n","  elapsed = time.time()-start\n","  print(\"Time: %s seconds\"%(elapsed))\n","\n","  #Sending mensage to slack\n","  web_hook_url = 'https://hooks.slack.com/services/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'\n","  slack_msg = {'text': 'Training was finished'}\n","  requests.post(web_hook_url,data = json.dumps(slack_msg))\n","\n","#model_training(vocab_size, embedding_matrix, embedding_dims, hidden_size, BATCH_SIZE, name_layer_encoder, name_layer_decoder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QoWalnsnO83n"},"source":["## Generate Summaries"]},{"cell_type":"code","metadata":{"id":"CYBRjx-7ABH_"},"source":["import pandas as pd\n","\n","def evaluate(sentence, encoder, decoder):\n","\n","  attention_plot = np.zeros((summary_max_len, article_max_len))\n","  input_lines = ['sostok '+sentence+''] \n","  \n","  #Adiciona token unktok nas palavras que não pertencem ao dicionário\n","  for line in input_lines:\n","    i = 0\n","    vet = []\n","    for w in line.split(' '):\n","      try:\n","        tokenizer.word_index[w]\n","        vet.append(w)\n","      except:\n","        vet.append('unktok')\n","\n","      i+=1\n","\n","  input_lines = [\" \".join(vet)]\n","  input_sequences = [[tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n","  input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,maxlen=article_max_len, padding='post')\n","  inputs = tf.convert_to_tensor(input_sequences)\n","\n","  result = ''\n","\n","  if(name_layer_encoder == \"LSTM\" or name_layer_encoder == \"GRU\"):\n","    hidden = (tf.zeros([1, hidden_size]),tf.zeros([1, hidden_size]))\n","  elif(name_layer_encoder == \"BiLSTM\" or name_layer_encoder == \"BiGRU\"):\n","    hidden = (tf.zeros([1, hidden_size]),tf.zeros([1, hidden_size]),tf.zeros([1, hidden_size]),tf.zeros([1, hidden_size]))\n","  enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","  dec_hidden = enc_hidden\n","  dec_input = tf.expand_dims([tokenizer.word_index['eostok']], 0)\n","\n","  for t in range(summary_max_len):\n","    predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                         dec_hidden,\n","                                                         enc_out)\n","\n","    # storing the attention weights to plot later on\n","    attention_weights = tf.reshape(attention_weights, (-1, ))\n","    attention_plot[t] = attention_weights.numpy()\n","\n","    predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","    result += tokenizer.index_word[predicted_id] + ' '\n","\n","    if tokenizer.index_word[predicted_id] == 'eostok':\n","      return result, sentence, attention_plot\n","\n","    # the predicted ID is fed back into the model\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","\n","  return result, sentence, attention_plot\n","\n","def plot_attention(attention, article, summary):\n","\n","  '''\n","      Função para gerar gráfico com os pesos do mecanismo de atenção\n","\n","      Inputs: \n","              attention: matriz com pesos de atenção\n","              article: abstract da patente\n","              summary: resumo gerado pelo modelo\n","\n","      Outputs: None\n","  '''\n","  \n","  fig = plt.figure(figsize=(30,30))\n","  ax = fig.add_subplot(1, 1, 1)\n","  ax.matshow(attention.T, cmap='viridis')\n","\n","  fontdict = {'fontsize': 14}\n","\n","  ax.set_xticklabels([''] + summary, fontdict=fontdict, rotation=90)\n","  ax.set_yticklabels([''] + article, fontdict=fontdict)\n","\n","  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","  plt.show()\n","\n","def attention_vector(attention_matrix):\n","  '''\n","    A partir dos valores de pesos de atençao, essa função cria um vetor que contém\n","    o peso geral das palavras de entrada para as palavras de saída\n","\n","    Input: \n","            attention_matrix = matriz de pesos com com quantidades de colunas\n","            iguais as quantidades de palavras de entrada\n","    Output: vetor com o peso geral de cada palavra\n","  '''\n","\n","  attention_values = []\n","  j = 0\n","\n","  for attn_vector in attention_matrix:\n","    i = 0\n","    #attn_word representa o peso de atenção de uma determinada palavra\n","    for attn_word in attn_vector:\n","      #Adiciona os primeiros valores de vetor de atenção\n","      if(j == 0):\n","        attention_values.append(attn_word)\n","      #Incrementa os valores de atenção\n","      else:\n","        attention_values[i] = attention_values[i] + attn_word\n","      i+=1\n","    j+=1\n","\n","  return attention_values\n","\n","def get_max_attention(attention_vector,article,n=20):\n","\n","  '''\n","      Função que seleciona os 'n' maiores valores de atenção\n","      \n","      Input: \n","            attention_vector = vetor com valores de atenção de cada palavra\n","            n = quantidade de palavras selecionas; default = 20\n","\n","      Output: vetor com os indices das 'n' palavras com maior valor de atenção\n","  '''\n","\n","  #Converte vetor de entrada\n","  vector = pd.Series(attention_vector)\n","  #Seleciona os 'n' maiores valores\n","  index_max_values = vector.nlargest(n)\n","  #Converte vetor em lista\n","  list_index_max_values = index_max_values.index.values.tolist()\n","\n","  words_max_values = []\n","\n","  #Seleciona as palavras de maior valor de atenção\n","  for i in list_index_max_values:\n","    if(article.split(\" \")[i] != \"#\"):\n","\n","      words_max_values.append(article.split(\" \")[i])\n","\n","  #Remove as palavras repetidas\n","  words_max_values = sorted(set(words_max_values))\n","\n","  return words_max_values\n","\n","def abstractive_summary(article, encoder, decoder):\n","\n","  '''\n","      Função para gerar resumo de saída\n","\n","      Input: Abstract da patente\n","      Output: Summary and attention_plot\n","  '''\n","\n","  summary, article_reference, attention_plot = evaluate(article, encoder, decoder)\n","\n","  attention_plot = attention_plot[:len(summary.split(' ')), :len(article_reference.split(' '))]\n","\n","  return attention_plot,summary\n","\n","def hybrid_summarizer(article,words_max_attention):\n","\n","  #Separa o texto em sentenças\n","  lines_article = article.split('.')\n","\n","  #Verifica a quantidade de interseções entre as palavras com maior valor de atenção\n","  # e cada sentença do texto de entrada\n","  count_intersections = []\n","  for i in lines_article:\n","    words_article = i.strip().split(' ')\n","    cont = 0\n","    for word in words_article:\n","      for word_max in words_max_attention:\n","        if(word_max == word):\n","          cont+=1\n","    count_intersections.append(cont)\n","\n","  return lines_article[count_intersections.index(max(count_intersections))]\n","\n","def generates_summaries(VALID, path_save, encoder, decoder):\n","\n","  #Load validation dataset\n","  base_article_valid = VALID.iloc[:,0].values\n","  base_summary_valid = VALID.iloc[:,1].values\n","\n","  hybrid_e1 = []\n","  hybrid_e2 = []\n","  hybrid_e3 = []\n","  article = []\n","  candidates_hybrid_summary = []\n","  candidates_abstractive_summary = []\n","  vet_words_max_attention = []\n","  references_summary = []\n","\n","  file_hybrid_e1 = open(path_save + \"summaries/e1/\" + number_model + \".txt\", \"w\")\n","  file_hybrid_e2 = open(path_save + \"summaries/e2/\" + number_model + \".txt\", \"w\")\n","  file_hybrid_e3 = open(path_save + \"summaries/e3/\" + number_model + \".txt\", \"w\")\n","\n","  print(\"Generating Summaries\")\n","\n","  valid_article_path = \"/content/drive/My Drive/Colab Notebooks/sumdata/database_scim_extend/with_stopwords/abstract.valid.pp.txt\"\n","  valid_title_path   = \"/content/drive/My Drive/Colab Notebooks/sumdata/database_scim_extend/with_stopwords/title.valid.pp.txt\"\n","\n","\n","  _,_,article_valid,summary_valid = load_files(train_article_path,\n","                                              train_title_path,\n","                                              valid_article_path,\n","                                              valid_title_path)\n","\n","  article_valid = article_valid.read().split('\\n')\n","  summary_valid = summary_valid.read().split('\\n')\n","\n","  for i in range(8306):\n","\n","    #Select sentence\n","    input_raw = base_article_valid[i].replace('eostok','').replace('sostok','').strip()\n","    #Abstrative summarization\n","    attention_matrix, candidate_abstractive_summary = abstractive_summary(input_raw, encoder, decoder)\n","    #Calcula vetor de atenção\n","    attention_vector_result = attention_vector(attention_matrix)\n","    #Seleciona as palavras com maior valor de atenção\n","    vet_words_max_attention = get_max_attention(attention_vector_result,input_raw,n=20)\n","\n","\n","    article.append(article_valid[i])\n","    aux = hybrid_summarizer(article_valid[i],vet_words_max_attention)\n","\n","    #print(\"\\nSummary Referene\")\n","    references_summary.append(summary_valid[i].replace('eostok','').replace('sostok',''))\n","\n","    candidate_hybrid_summary = ' '.join(list(filter(None, aux.rstrip().strip().split(\" \")))[:])\n","    e1 = candidate_hybrid_summary.replace('eostok','').replace('sostok','').strip()\n","    hybrid_e1.append(e1)\n","    file_hybrid_e1.write(e1 + \"\\n\")\n","\n","    #estratégia 2\n","    candidate_hybrid_summary = ' '.join(list(filter(None, aux.rstrip().strip().split(\" \")))[:15])\n","    e2 = candidate_hybrid_summary.replace('eostok','').replace('sostok','').strip()\n","    hybrid_e2.append(e2)\n","    file_hybrid_e2.write(e2 + \"\\n\")\n","\n","    #estratégia 3\n","    len_output = int((len(input_raw.split(\" \")[:150])*0.1)+0.5)\n","    candidate_hybrid_summary = ' '.join(list(filter(None, aux.rstrip().strip().split(\" \")))[:len_output])\n","    e3 = candidate_hybrid_summary.replace('eostok','').replace('sostok','').strip()\n","    hybrid_e3.append(e3)\n","    file_hybrid_e3.write(e3 + \"\\n\")\n","\n","\n","    #print(\"\\nAbstractive summary: \")\n","    candidates_abstractive_summary.append(candidate_abstractive_summary.replace('eostok','').replace('sostok',''))\n","    #print(candidate_abstractive_summary)\n","\n","  file_hybrid_e1.close()\n","  file_hybrid_e2.close()\n","  file_hybrid_e3.close()\n","  print(\"Fim da geração de resumos\")\n","\n","  #Sending mensage to slack\n","  web_hook_url = 'https://hooks.slack.com/services/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'\n","  slack_msg = {'text': 'The Generation of Sentences was finished'}\n","  requests.post(web_hook_url,data = json.dumps(slack_msg))\n","\n","  return hybrid_e1, hybrid_e2, hybrid_e3, candidates_abstractive_summary\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tzmgp6N-1qgR"},"source":["# Training\n"]},{"cell_type":"code","metadata":{"id":"yro1btuyqLnS"},"source":["# Paths\n","train_article_path = \"/content/drive/My Drive/Colab Notebooks/sumdata/database_scim_extend/without_stopwords/abstract.train.pp.txt\"\n","train_title_path   = \"/content/drive/My Drive/Colab Notebooks/sumdata/database_scim_extend/without_stopwords/title.train.pp.txt\"\n","valid_article_path = \"/content/drive/My Drive/Colab Notebooks/sumdata/database_scim_extend/without_stopwords/abstract.valid.pp.txt\"\n","valid_title_path   = \"/content/drive/My Drive/Colab Notebooks/sumdata/database_scim_extend/without_stopwords/title.valid.pp.txt\"\n","\n","BATCH_SIZE = 64\n","embedding_dims = 300\n","hidden_size  = 150\n","learning_rate = 0.001\n","article_max_len = 150 + 2 #Tamanho maximo do texto de entrada\n","summary_max_len = 15 + 2 #Tamanho maximo do texto de saida\n","number_model = 'model_14'\n","dropout=0.4\n","mode='VALID'\n","\n","name_layer_encoder = \"BiLSTM\"\n","name_layer_decoder = \"LSTM\"\n","\n","path_save = '/content/drive/My Drive/Colab Notebooks/Hybrid_Summ_App/approach_2/'\n","\n","if not os.path.exists(path_save + number_model):\n","    print(\"Create a new directory\")\n","    os.mkdir(path_save + number_model)\n","\n","dataset, VALID, steps_per_epoch, tokenizer, vocab_size = data_preparation(mode, number_model, train_article_path, train_title_path,\n","                 valid_article_path, valid_title_path, path_save=path_save, article_max_len=article_max_len,\n","                 summary_max_len=summary_max_len, verbose=True)\n","\n","embedding_matrix = create_embeddings(tokenizer, article_max_len, vocab_size, path_save=path_save, number_model=number_model, verbose=False)\n","\n","\n","#model_training(vocab_size, embedding_matrix, embedding_dims, hidden_size, BATCH_SIZE, name_layer_encoder, name_layer_decoder, path_save, steps_per_epoch, dropout)\n","\n","#Plot attention weights\n","#attention_plot = attention_matrix\n","#plot_attention(attention_plot, input_raw.split(' '), candidate_abstractive_summary.split(' '))\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n250XbnjOaqP"},"source":["## Restore the latest checkpoint and test"]},{"cell_type":"code","metadata":{"id":"s6DDwANzqTLb"},"source":["encoder = Encoder(vocab_size, embedding_matrix, embedding_dims, dropout, hidden_size, BATCH_SIZE, name_layer_encoder)\n","decoder = Decoder(vocab_size, embedding_matrix, embedding_dims, hidden_size, BATCH_SIZE, name_layer_decoder, name_layer_encoder)\n","checkpoint_dir = path_save + number_model + '/'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)\n","manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)\n","\n","print(\"Restored from {}\".format(manager.latest_checkpoint))\n","checkpoint.restore(manager.latest_checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTP27KnCWjZj"},"source":["base_article_valid = VALID.iloc[:,0].values\n","input_raw = base_article_valid[1212].replace('eostok','').replace('sostok','').strip()\n","print(input_raw)\n","#Abstrative summarization\n","attention_matrix, candidate_abstractive_summary = abstractive_summary(input_raw, encoder, decoder)\n","#Calcula vetor de atenção\n","attention_vector_result = attention_vector(attention_matrix)\n","vet_words_max_attention = get_max_attention(attention_vector_result,input_raw,n=20)\n","print(vet_words_max_attention)\n","plot_attention(attention_matrix, input_raw.split(' '), candidate_abstractive_summary.split(' '))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypwphVWO77B8"},"source":["#hybrid_e1, hybrid_e2, hybrid_e3, candidates_abstractive_summary = generates_summaries(VALID, path_save, encoder, decoder)                                  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t1AXiHXB8GEQ"},"source":["# Evaluate model using ROUGEs e NUBIA metrics"]},{"cell_type":"code","metadata":{"id":"vstBMlcxLZuZ"},"source":["cd /content/drive/My Drive/Colab Notebooks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9p6yW_kt8FFR"},"source":["'''\n","!git clone https://github.com/google-research/bleurt.git\n","os.chdir('bleurt')\n","!pip install .\n","from bleurt import score\n","tf.compat.v1.flags.DEFINE_string('f','','')\n","checkpoint = \"bleurt/test_checkpoint\"\n","bleurt = score.BleurtScorer(checkpoint)\n","'''\n","\n","#!git clone https://github.com/wl-research/nubia.git\n","os.chdir('nubia')\n","!pip install -r requirements.txt\n","from nubia import Nubia\n","nubia = Nubia()\n","\n","!pip install sumeval\n","!python -m spacy download en"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mWIGqzB8MSK"},"source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","from sumeval.metrics.rouge import RougeCalculator\n","from nubia import Nubia\n","from xml.etree import ElementTree\n","from xml.dom import minidom\n","from functools import reduce\n","from xml.etree.ElementTree import Element, SubElement, Comment\n","\n","\n","def eval(\n","    reference_summary, model_summary, metrics=[\"ROUGE_1\", \"ROUGE_2\", \"ROUGE_L\", \"NUBIA\", \"BLEURT\"]):\n","\n","    rouge = RougeCalculator(stopwords=True, lang=\"en\")\n","\n","    if(\"ROUGE_1\" in metrics):\n","      rouge_1 = rouge.rouge_n( summary=model_summary, references=reference_summary, n=1)\n","    else:\n","      rouge_1 = None\n","\n","    if(\"ROUGE_2\" in metrics):\n","      rouge_2 = rouge.rouge_n(summary=model_summary,references=[reference_summary],n=2)\n","    else:\n","      rouge_2 = None\n","\n","    if(\"ROUGE_L\" in metrics):\n","      rouge_l = rouge.rouge_l( summary=model_summary,references=[reference_summary])\n","    else:\n","      rouge_l = None\n","\n","    if(\"NUBIA\" in metrics):\n","      nubia_score = nubia.score(reference_summary, model_summary)\n","    else:\n","      nubia_score =  None\n","\n","    if(\"BLEURT\" in metrics):\n","      bleurt_score = scorer.score([reference_summary], [model_summary])\n","      assert type(bleurt_score) == list and len(bleurt_score) == 1\n","    else:\n","      bleurt_score = None\n","\n","    return rouge_1, rouge_2,rouge_l, nubia_score, bleurt_score\n","\n","def prettify(elem):\n","      \"\"\"Return a pretty-printed XML string for the Element.\n","      \"\"\"\n","      rough_string = ElementTree.tostring(elem, 'utf-8')\n","      reparsed = minidom.parseString(rough_string)\n","      return reparsed.toprettyxml(indent=\"  \")\n","  \n","def create_report_valid(\n","    summary_array, references_summary, article, name_file,\n","     metrics=[\"ROUGE_1\", \"ROUGE_2\", \"ROUGE_L\", \"NUBIA\", \"BLEURT\"]):\n","\n","  rouge_1_arr  = []\n","  rouge_2_arr  = []\n","  rouge_L_arr  = []\n","  NUBIA_arr = []\n","  bleurt_arr = []\n","\n","  top = Element('ZakSum')\n","\n","  comment = Comment('Generated by Amr Zaki')\n","  top.append(comment)\n","\n","  i=0\n","  for summ in summary_array:\n","\n","      \n","      example = SubElement(top, 'example')\n","      article_element   = SubElement(example, 'article')\n","      article_element.text = article[i]\n","  \n","      reference_element = SubElement(example, 'reference')\n","      reference_element.text = references_summary[i]\n","  \n","      summary_element   = SubElement(example, 'summary')\n","      summary_element.text = summ\n","\n","      if(len(summ) != 0):\n","        rouge_1, rouge_2, rouge_L, nubia_score, bleurt_score = eval(references_summary[i],summ, metrics=metrics )\n","      else: \n","        rouge_1 = rouge_2 = rouge_L = nubia_score, bleurt_score = 0\n","  \n","      eval_element = SubElement(example, 'eval')\n","      if(rouge_1 != None):\n","        ROUGE_1_element  = SubElement(eval_element, 'ROUGE_1' , {'score':str(rouge_1)})\n","        rouge_1_arr.append(rouge_1) \n","      if(rouge_2 != None):\n","        ROUGE_2_element  = SubElement(eval_element, 'ROUGE_2' , {'score':str(rouge_2)})\n","        rouge_2_arr.append(rouge_2)\n","      if(rouge_L != None):\n","        ROUGE_L_element  = SubElement(eval_element, 'ROUGE_l' , {'score':str(rouge_L)})\n","        rouge_L_arr.append(rouge_L)\n","      if(nubia_score != None): \n","        NUBIA_element =  SubElement(eval_element,'NUBIA', {'score':str(nubia_score)})\n","        NUBIA_arr.append(nubia_score)\n","      if(bleurt_score != None): \n","        BLEURT_element =  SubElement(eval_element,'BLEURT', {'score':str(bleurt_score[0])})\n","        bleurt_arr.append(bleurt_score[0])\n","  \n","      i+=1\n","\n","  if(rouge_1_arr != []): top.set('rouge_1', str(np.mean(rouge_1_arr)))\n","  if(rouge_2_arr != []): top.set('rouge_2', str(np.mean(rouge_2_arr)))\n","  if(rouge_L_arr != []): top.set('rouge_L', str(np.mean(rouge_L_arr)))\n","  if(NUBIA_arr != []): top.set('NUBIA', str(np.mean(NUBIA_arr)))\n","  if(bleurt_arr != []):top.set('BLEURT', str(np.mean(bleurt_arr)))\n","\n","\n","  with open(name_file, \"w+\") as f:\n","    print(prettify(top), file=f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"enZj1ExKnNjo"},"source":["#Load summaries generated\n","hybrid_e3 = open(path_save + \"summaries/e1/\" + number_model + \".txt\").readlines()\n","hybrid_e3 = [i.replace(\"\\n\",\"\") for i in hybrid_e3]\n","\n","#Load Files not preprocess\n","_,_,article_valid,summary_valid = load_files(train_article_path,\n","                                             train_title_path,\n","                                             valid_article_path,\n","                                             valid_title_path)\n","article = article_valid.read().split('\\n')\n","references_summary = summary_valid.read().split('\\n')\n","\n","\n","#Create validation reporton\"\n","metrics=[\"ROUGE_1\", \"ROUGE_2\", \"ROUGE_L\", \"NUBIA\"]\n","create_report_valid(\n","    hybrid_e3, references_summary, article, name_file=\"{}{}/{}/{}\".format(path_save,\"validation\", \"e3\", number_model + \".xml\" ), metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qas9GG2ROxlf"},"source":["#Sending mensage to slack\n","web_hook_url = 'https://hooks.slack.com/services/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'\n","slack_msg = {'text': 'Validation of Sentences was finished'}\n","requests.post(web_hook_url,data = json.dumps(slack_msg))"],"execution_count":null,"outputs":[]}]}