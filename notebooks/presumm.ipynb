{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a3be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import torch\n",
    "from multiprocess import Pool\n",
    "\n",
    "from others.logging import logger\n",
    "from others.tokenization import BertTokenizer\n",
    "from pytorch_transformers import XLNetTokenizer\n",
    "\n",
    "from others.utils import clean\n",
    "from prepro.utils import _get_word_ngrams\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "nyt_remove_words = [\"photo\", \"graph\", \"chart\", \"map\", \"table\", \"drawing\"]\n",
    "\n",
    "def load_json(p, lower):\n",
    "    source = []\n",
    "    tgt = []\n",
    "    flag = False\n",
    "    for sent in json.load(open(p))['sentences']:\n",
    "        tokens = [t['word'] for t in sent['tokens']]\n",
    "        if (lower):\n",
    "            tokens = [t.lower() for t in tokens]\n",
    "        if (tokens[0] == '@highlight'):\n",
    "            flag = True\n",
    "            tgt.append([])\n",
    "            continue\n",
    "        if (flag):\n",
    "            tgt[-1].extend(tokens)\n",
    "        else:\n",
    "            source.append(tokens)\n",
    "\n",
    "    source = [clean(' '.join(sent)).split() for sent in source]\n",
    "    tgt = [clean(' '.join(sent)).split() for sent in tgt]\n",
    "    return source, tgt\n",
    "\n",
    "\n",
    "def greedy_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "\n",
    "    max_rouge = 0.0\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    selected = []\n",
    "    for s in range(summary_size):\n",
    "        cur_max_rouge = max_rouge\n",
    "        cur_id = -1\n",
    "        for i in range(len(sents)):\n",
    "            if (i in selected):\n",
    "                continue\n",
    "            c = selected + [i]\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "            rouge_score = rouge_1 + rouge_2\n",
    "            if rouge_score > cur_max_rouge:\n",
    "                cur_max_rouge = rouge_score\n",
    "                cur_id = i\n",
    "        if (cur_id == -1):\n",
    "            return selected\n",
    "        selected.append(cur_id)\n",
    "        max_rouge = cur_max_rouge\n",
    "\n",
    "    return sorted(selected)\n",
    "\n",
    "\n",
    "class BertData():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.tgt_bos = '[unused0]'\n",
    "        self.tgt_eos = '[unused1]'\n",
    "        self.tgt_sent_split = '[unused2]'\n",
    "        self.sep_vid = self.tokenizer.vocab[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.vocab[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.vocab[self.pad_token]\n",
    "\n",
    "    def preprocess(self, src, tgt, sent_labels, use_bert_basic_tokenizer=False, is_test=False):\n",
    "\n",
    "        if ((not is_test) and len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [' '.join(s) for s in src]\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > self.args.min_src_ntokens_per_sent)]\n",
    "\n",
    "        _sent_labels = [0] * len(src)\n",
    "        for l in sent_labels:\n",
    "            _sent_labels[l] = 1\n",
    "\n",
    "        src = [src[i][:self.args.max_src_ntokens_per_sent] for i in idxs]\n",
    "        sent_labels = [_sent_labels[i] for i in idxs]\n",
    "        src = src[:self.args.max_src_nsents]\n",
    "        sent_labels = sent_labels[:self.args.max_src_nsents]\n",
    "\n",
    "        if ((not is_test) and len(src) < self.args.min_src_nsents):\n",
    "            return None\n",
    "\n",
    "        src_txt = [' '.join(sent) for sent in src]\n",
    "        text = ' {} {} '.format(self.sep_token, self.cls_token).join(src_txt)\n",
    "\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        src_subtokens = [self.cls_token] + src_subtokens + [self.sep_token]\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        sent_labels = sent_labels[:len(cls_ids)]\n",
    "\n",
    "        tgt_subtokens_str = '[unused0] ' + ' [unused2] '.join(\n",
    "            [' '.join(self.tokenizer.tokenize(' '.join(tt), use_bert_basic_tokenizer=use_bert_basic_tokenizer)) for tt in tgt]) + ' [unused1]'\n",
    "        tgt_subtoken = tgt_subtokens_str.split()[:self.args.max_tgt_ntokens]\n",
    "        if ((not is_test) and len(tgt_subtoken) < self.args.min_tgt_ntokens):\n",
    "            return None\n",
    "\n",
    "        tgt_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(tgt_subtoken)\n",
    "\n",
    "        tgt_txt = '<q>'.join([' '.join(tt) for tt in tgt])\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "\n",
    "        return src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt\n",
    "\n",
    "\n",
    "def format_to_bert(args):\n",
    "    if (args.dataset != ''):\n",
    "        datasets = [args.dataset]\n",
    "    else:\n",
    "        datasets = ['train', 'valid', 'test']\n",
    "    for corpus_type in datasets:\n",
    "        a_lst = []\n",
    "        for json_f in glob.glob(pjoin(args.raw_path, '*' + corpus_type + '.*.json')):\n",
    "            real_name = json_f.split('/')[-1]\n",
    "            a_lst.append((corpus_type, json_f, args, pjoin(args.save_path, real_name.replace('json', 'bert.pt'))))\n",
    "        print(a_lst)\n",
    "        pool = Pool(args.n_cpus)\n",
    "        for d in pool.imap(_format_to_bert, a_lst):\n",
    "            pass\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "\n",
    "def _format_to_bert(params):\n",
    "    corpus_type, json_file, args, save_file = params\n",
    "    is_test = corpus_type == 'test'\n",
    "    if (os.path.exists(save_file)):\n",
    "        logger.info('Ignore %s' % save_file)\n",
    "        return\n",
    "\n",
    "    bert = BertData(args)\n",
    "\n",
    "    logger.info('Processing %s' % json_file)\n",
    "    jobs = json.load(open(json_file))\n",
    "    datasets = []\n",
    "    for d in jobs:\n",
    "        source, tgt = d['src'], d['tgt']\n",
    "\n",
    "        sent_labels = greedy_selection(source[:args.max_src_nsents], tgt, 3)\n",
    "        if (args.lower):\n",
    "            source = [' '.join(s).lower().split() for s in source]\n",
    "            tgt = [' '.join(s).lower().split() for s in tgt]\n",
    "        b_data = bert.preprocess(source, tgt, sent_labels, use_bert_basic_tokenizer=args.use_bert_basic_tokenizer,\n",
    "                                 is_test=is_test)\n",
    "\n",
    "        if (b_data is None):\n",
    "            continue\n",
    "        src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
    "        b_data_dict = {\"src\": src_subtoken_idxs, \"tgt\": tgt_subtoken_idxs,\n",
    "                       \"src_sent_labels\": sent_labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "                       'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
    "        datasets.append(b_data_dict)\n",
    "    logger.info('Processed instances %d' % len(datasets))\n",
    "    logger.info('Saving to %s' % save_file)\n",
    "    torch.save(datasets, save_file)\n",
    "    datasets = []\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def format_to_lines(args):\n",
    "    corpus_mapping = {}\n",
    "    for corpus_type in ['valid', 'test', 'train']:\n",
    "        temp = []\n",
    "        for line in open(pjoin(args.map_path, 'mapping_' + corpus_type + '.txt')):\n",
    "            temp.append(hashhex(line.strip()))\n",
    "        corpus_mapping[corpus_type] = {key.strip(): 1 for key in temp}\n",
    "    train_files, valid_files, test_files = [], [], []\n",
    "    for f in glob.glob(pjoin(args.raw_path, '*.json')):\n",
    "        real_name = f.split('/')[-1].split('.')[0]\n",
    "        if (real_name in corpus_mapping['valid']):\n",
    "            valid_files.append(f)\n",
    "        elif (real_name in corpus_mapping['test']):\n",
    "            test_files.append(f)\n",
    "        elif (real_name in corpus_mapping['train']):\n",
    "            train_files.append(f)\n",
    "\n",
    "    corpora = {'train': train_files, 'valid': valid_files, 'test': test_files}\n",
    "    for corpus_type in ['train', 'valid', 'test']:\n",
    "        a_lst = [(f, args) for f in corpora[corpus_type]]\n",
    "        pool = Pool(args.n_cpus)\n",
    "        dataset = []\n",
    "        p_ct = 0\n",
    "        for d in pool.imap_unordered(_format_to_lines, a_lst):\n",
    "            dataset.append(d)\n",
    "            if (len(dataset) > args.shard_size):\n",
    "                pt_file = \"{:s}.{:s}.{:d}.json\".format(args.save_path, corpus_type, p_ct)\n",
    "                with open(pt_file, 'w') as save:\n",
    "                    save.write(json.dumps(dataset))\n",
    "                    p_ct += 1\n",
    "                    dataset = []\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        if (len(dataset) > 0):\n",
    "            pt_file = \"{:s}.{:s}.{:d}.json\".format(args.save_path, corpus_type, p_ct)\n",
    "            with open(pt_file, 'w') as save:\n",
    "                save.write(json.dumps(dataset))\n",
    "                p_ct += 1\n",
    "                dataset = []\n",
    "                \n",
    "def _format_to_lines(params):\n",
    "    f, args = params\n",
    "    source, tgt = load_json(f, args.lower)\n",
    "    return {'src': source, 'tgt': tgt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2078fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_format_to_lines(args):\n",
    "    corpus_mapping = {}\n",
    "    files = []\n",
    "    dataset = []\n",
    "\n",
    "    for f in glob.glob(pjoin(args.raw_path, '*.json')):\n",
    "        files.append(f)\n",
    "    \n",
    "    corpora = {'test': files}    \n",
    "    for corpus_type in ['test']:\n",
    "        for file in files:\n",
    "            dataset.append(_format_to_lines([file, args]))\n",
    "            pt_file = \"{:s}.{:s}.{:s}.json\".format(args.save_path, corpus_type, file[36:-11])\n",
    "    \n",
    "            with open(pt_file, 'w') as save:\n",
    "                save.write(json.dumps(dataset))\n",
    "                dataset = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f54f58",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aba431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "# Download the src folder from the repo https://github.com/nlpyang/PreSumm/tree/master/src, \n",
    "# placing it on the current directory in order to access the `other` and `prepro` modules\n",
    "from others.logging import init_logger\n",
    "from prepro import data_builder\n",
    "\n",
    "def do_tokenize(args):\n",
    "    print(time.clock())\n",
    "    data_builder.tokenize(args)\n",
    "    print(time.clock())\n",
    "    \n",
    "def do_format_to_lines(args):\n",
    "    print(time.clock())\n",
    "    custom_format_to_lines(args)\n",
    "    print(time.clock())\n",
    "\n",
    "def do_format_to_bert(args):\n",
    "    print(time.clock())\n",
    "    format_to_bert(args)\n",
    "    print(time.clock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e693d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "\n",
    "args.mode='tokenize'\n",
    "args.oracle_mode='greedy'\n",
    "args.map_path='./urls'\n",
    "args.save_path='./tokenized/'\n",
    "args.raw_path='./raw_data/'\n",
    "args.shard_size=2000\n",
    "args.min_nsents=3\n",
    "args.max_nsents=100\n",
    "args.min_src_ntokens=5\n",
    "args.max_src_ntokens=200\n",
    "args.lower=True\n",
    "args.log_file='./logs/cnndm.log'\n",
    "args.dataset='test'\n",
    "args.n_cpus=1\n",
    "\n",
    "do_tokenize(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7abca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "\n",
    "args.mode='do_format_to_lines'\n",
    "args.oracle_mode='greedy'\n",
    "args.map_path='../urls'\n",
    "args.save_path='./json_data/'\n",
    "args.raw_path='./tokenized/'\n",
    "args.shard_size=2000\n",
    "args.min_nsents=3\n",
    "args.max_nsents=100\n",
    "args.min_src_ntokens=5\n",
    "args.max_src_ntokens=200\n",
    "args.lower=True\n",
    "args.log_file='./logs/cnndm.log'\n",
    "args.dataset='test'\n",
    "args.n_cpus=1\n",
    "\n",
    "\n",
    "do_format_to_lines(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced810ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    pass\n",
    "\n",
    "args.mode='do_format_to_bert'\n",
    "args.oracle_mode='greedy'\n",
    "args.map_path='../urls'\n",
    "args.save_path='./bert_data/'\n",
    "args.raw_path='./json_data/'\n",
    "args.shard_size=2000\n",
    "args.min_src_nsents=3\n",
    "args.max_src_nsents=100\n",
    "args.min_src_ntokens_per_sent=5\n",
    "args.max_src_ntokens_per_sent=200\n",
    "args.lower=True\n",
    "args.log_file='./logs/cnndm.log'\n",
    "args.use_bert_basic_tokenizer=False\n",
    "args.dataset='test'\n",
    "args.n_cpus=2\n",
    "args.min_tgt_ntokens=5\n",
    "args.max_tgt_ntokens=500\n",
    "\n",
    "do_format_to_bert(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb196f6",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d0b763",
   "metadata": {},
   "source": [
    "### BertSumExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a945ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 src/train.py -report_rouge=False -mode test -encoder bert -task ext -test_from ./models/bertext_cnndm_transformer.pt -bert_data_path ./bert_data -log_file ./logs/bertext_cnndm_transformer -result_path='./results/cnndm'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa8fb75",
   "metadata": {},
   "source": [
    "### BertSumExtAbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 src/train.py -report_rouge=False -mode test -task abs -test_from ./models/cnn_abs/model_step_148000.pt -model_path ./models/cnn_abs -bert_data_path ./bert_data -log_file ./logs/bertext_cnndm_abs_transformer -result_path='./results/cnndm_abs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95035f",
   "metadata": {},
   "source": [
    "### TransformerAbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a822d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 src/train.py -report_rouge=False -mode test -task abs -test_from ./models/cnn_abs_baseline/cnndm_baseline_best.pt -model_path ./models/cnn_abs_baseline -bert_data_path ./bert_data -log_file ./logs/cnn_abs_baseline -result_path='./results/cnndm_abs_baseline'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc00ae",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/wl-research/nubia.git\n",
    "os.chdir('nubia')\n",
    "#!pip install -r requirements.txt\n",
    "from nubia_score import Nubia\n",
    "nubia = Nubia()\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89c009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "from xml.etree import ElementTree\n",
    "from xml.dom import minidom\n",
    "from functools import reduce\n",
    "from xml.etree.ElementTree import Element, SubElement, Comment\n",
    "\n",
    "\n",
    "def eval(\n",
    "    reference_summary, model_summary, metrics=[\"ROUGE_1\", \"ROUGE_2\", \"ROUGE_L\", \"NUBIA\", \"BLEURT\"]):\n",
    "\n",
    "    rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
    "\n",
    "    if(\"ROUGE_1\" in metrics):\n",
    "      rouge_1 = rouge.rouge_n( summary=model_summary, references=reference_summary, n=1)\n",
    "    else:\n",
    "      rouge_1 = None\n",
    "\n",
    "    if(\"ROUGE_2\" in metrics):\n",
    "      rouge_2 = rouge.rouge_n(summary=model_summary,references=[reference_summary],n=2)\n",
    "    else:\n",
    "      rouge_2 = None\n",
    "\n",
    "    if(\"ROUGE_L\" in metrics):\n",
    "      rouge_l = rouge.rouge_l( summary=model_summary,references=[reference_summary])\n",
    "    else:\n",
    "      rouge_l = None\n",
    "\n",
    "    if(\"NUBIA\" in metrics):\n",
    "      nubia_score = nubia.score(reference_summary, model_summary)\n",
    "    else:\n",
    "      nubia_score =  None\n",
    "\n",
    "    if(\"BLEURT\" in metrics):\n",
    "      bleurt_score = scorer.score([reference_summary], [model_summary])\n",
    "      assert type(bleurt_score) == list and len(bleurt_score) == 1\n",
    "    else:\n",
    "      bleurt_score = None\n",
    "\n",
    "    return rouge_1, rouge_2,rouge_l, nubia_score, bleurt_score\n",
    "\n",
    "def prettify(elem):\n",
    "      \"\"\"Return a pretty-printed XML string for the Element.\n",
    "      \"\"\"\n",
    "      rough_string = ElementTree.tostring(elem, 'utf-8')\n",
    "      reparsed = minidom.parseString(rough_string)\n",
    "      return reparsed.toprettyxml(indent=\"  \")\n",
    "  \n",
    "def create_report_valid(\n",
    "    summary_array, references_summary, article, name_file,\n",
    "     metrics=[\"ROUGE_1\", \"ROUGE_2\", \"ROUGE_L\", \"NUBIA\", \"BLEURT\"]):\n",
    "\n",
    "  rouge_1_arr  = []\n",
    "  rouge_2_arr  = []\n",
    "  rouge_L_arr  = []\n",
    "  NUBIA_arr = []\n",
    "  bleurt_arr = []\n",
    "\n",
    "  top = Element('ZakSum')\n",
    "\n",
    "  comment = Comment('Generated by Amr Zaki')\n",
    "  top.append(comment)\n",
    "\n",
    "  i=0\n",
    "  for summ in summary_array:\n",
    "\n",
    "      \n",
    "      example = SubElement(top, 'example')\n",
    "      article_element   = SubElement(example, 'article')\n",
    "      article_element.text = article[i]\n",
    "  \n",
    "      reference_element = SubElement(example, 'reference')\n",
    "      reference_element.text = references_summary[i]\n",
    "  \n",
    "      summary_element   = SubElement(example, 'summary')\n",
    "      summary_element.text = summ\n",
    "\n",
    "      if(len(summ) != 0):\n",
    "        rouge_1, rouge_2, rouge_L, nubia_score, bleurt_score = eval(references_summary[i],summ, metrics=metrics )\n",
    "      else: \n",
    "        rouge_1 = rouge_2 = rouge_L = nubia_score, bleurt_score = 0\n",
    "  \n",
    "      eval_element = SubElement(example, 'eval')\n",
    "      if(rouge_1 != None):\n",
    "        ROUGE_1_element  = SubElement(eval_element, 'ROUGE_1' , {'score':str(rouge_1)})\n",
    "        rouge_1_arr.append(rouge_1) \n",
    "      if(rouge_2 != None):\n",
    "        ROUGE_2_element  = SubElement(eval_element, 'ROUGE_2' , {'score':str(rouge_2)})\n",
    "        rouge_2_arr.append(rouge_2)\n",
    "      if(rouge_L != None):\n",
    "        ROUGE_L_element  = SubElement(eval_element, 'ROUGE_l' , {'score':str(rouge_L)})\n",
    "        rouge_L_arr.append(rouge_L)\n",
    "      if(nubia_score != None): \n",
    "        NUBIA_element =  SubElement(eval_element,'NUBIA', {'score':str(nubia_score)})\n",
    "        NUBIA_arr.append(nubia_score)\n",
    "      if(bleurt_score != None): \n",
    "        BLEURT_element =  SubElement(eval_element,'BLEURT', {'score':str(bleurt_score[0])})\n",
    "        bleurt_arr.append(bleurt_score[0])\n",
    "  \n",
    "      i+=1\n",
    "\n",
    "  if(rouge_1_arr != []): top.set('rouge_1', str(np.mean(rouge_1_arr)))\n",
    "  if(rouge_2_arr != []): top.set('rouge_2', str(np.mean(rouge_2_arr)))\n",
    "  if(rouge_L_arr != []): top.set('rouge_L', str(np.mean(rouge_L_arr)))\n",
    "  if(NUBIA_arr != []): top.set('NUBIA', str(np.mean(NUBIA_arr)))\n",
    "  if(bleurt_arr != []):top.set('BLEURT', str(np.mean(bleurt_arr)))\n",
    "\n",
    "\n",
    "  with open(name_file, \"w+\") as f:\n",
    "    print(prettify(top), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03ad70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(candidates, references, sources, algorithm):\n",
    "    metrics=[\"ROUGE_1\", \"ROUGE_2\", \"ROUGE_L\", \"BLEU\", \"NUBIA\"]\n",
    "    rouge.create_report_valid(\n",
    "            candidates, references, sources,\n",
    "            name_file=\"./validation/{}1.xml\".format(algorithm),\n",
    "            metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acb215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the candidates texts are generated, they do not follow the same \n",
    "# order as the original summary and title. They are processed in the \n",
    "# following order: 0, 1, 10, 100, ...., 2, and not 0, 1, 2, 3, ...., which means\n",
    "# that when we call `evaluation` with the texts, the lists will have a different ordering,\n",
    "# resulting in wrong comparisions and results.\n",
    "#\n",
    "# The function below creates a new candidates file containing the content of a list where each\n",
    "# item is a two item list, with the first item as the patent number (id).\n",
    "# like, [[0, <patent0>], [1, <patent1>], [10, <patent2>], ...] and the second as the generated summary.\n",
    "# This is achieved by simpling listing the json files and extracting\n",
    "# the patent id from the name (this could perhaps be done with an int\n",
    "# range where each item is casted to string and then all sorted). After, we sort the\n",
    "# list by the id, resulting in an ordering just like the original summaries and titles:\n",
    "# [[0, <patent0>], [1, <patent1>], [2, <patent2>], ...]\n",
    "#\n",
    "# The new file will be the original name + \".sorted\", e.g. \"bert_cnn.candidate.sorted\"\n",
    "def sort_candidate_files(file):\n",
    "    ids = list(map(lambda f: f[15:-5], sorted(os.listdir(\"./json_data\"))))\n",
    "    candidates = open(file, \"r\").readlines()\n",
    "    zip_id_cans = list(zip(ids, candidates))\n",
    "    sorted_cans = list(sorted(zip_id_cans, key=lambda x: int(x[0])))\n",
    "    sorted_cans_texts = [can[1] for can in sorted_cans]\n",
    "    \n",
    "    f = open(file + \".sorted\", \"a\")\n",
    "    f.writelines(sorted_cans_texts)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_candidate_file(\"./results/cnndm_baseline.0.candidate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59861b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "patents = open(\"./resumo.valid.txt\").readlines()\n",
    "titles = open(\"./titulo.valid.txt\").readlines()\n",
    "candidates_texts = open(\"cnndm_baseline.0.candidate.sorted\", \"r\").readlines()\n",
    "evaluation(candidates_texts, titles, patents, 'bert_cnn_baseline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
