{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Validation_pos_predict.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JzF_h-dlEqcC","colab_type":"text"},"source":["\n","# **Validation of summary results**\n","\n","**Create by** Cinthia M. Souza\n","\n","**Created on** Tue Nov 26 12:29:16 2020\n","\n","This notebook was created with the intention of being a post prediction validation tool. Considering that you already have an .xml, in the appropriate format, it reads the .xml file loads the information from each summary and performs the validation with the selected metrics. At the end, a new .xml is generated that has the same format as the input .xml, however, with the results of the new metrics.\n","\n","If you want to calculate all metrics again, simply identify that you do not have any pre-calculated metrics. Thus, only the information regarding the entry summary to the reference and candidate summary will be loaded.\n","\n","So far, the metrics that can be used are: ROUGES, NUBIA and BLEURT. The articles that each metrics proposes are presented in the last cell of the notebook.\n","\n",".xml format:\n","\n","```\n","<?xml version=\"1.0\" ?>\n","<ZakSum BLEURT=\"0.0\" NUBIA=\"0.0\" rouge_1=\"0.0\" rouge_2=\"0.0\" rouge_L=\"0.\">\n","  <!--Generated by Amr Zaki-->\n","  <example>\n","    <article>Here is the input text </article>\n","    <reference>Here is the reference summary</reference>\n","    <summary>Here is the candidate summary</summary>\n","    <eval>\n","      <ROUGE_1 score=\"0.0\"/>\n","      <ROUGE_2 score=\"0.0\"/>\n","      <ROUGE_l score=\"0.0\"/>\n","      <NUBIA score=\"0.0\"/>\n","      <BLEURT score=\"-0.0\"/>\n","    </eval>\n","  </example>\n","</ZakSum>\n","```"]},{"cell_type":"code","metadata":{"id":"zUIPkxIfzrh5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1599045940987,"user_tz":180,"elapsed":28312,"user":{"displayName":"Cinthia Mikaela de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSYWVyWfXKPRe-m_hW9EaymViE56Sd6-54Bvh6=s64","userId":"06359001126903823254"}},"outputId":"95d992db-b67d-4453-995a-d3f6378edbb9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ATItTxTDpH6t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599045940990,"user_tz":180,"elapsed":28288,"user":{"displayName":"Cinthia Mikaela de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSYWVyWfXKPRe-m_hW9EaymViE56Sd6-54Bvh6=s64","userId":"06359001126903823254"}},"outputId":"ad567964-32b6-4f4a-a6df-4e3b6adfd1ef"},"source":["cd '/content/drive/My Drive/Colab Notebooks'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jFButU3ANP1a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599046133819,"user_tz":180,"elapsed":221106,"user":{"displayName":"Cinthia Mikaela de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSYWVyWfXKPRe-m_hW9EaymViE56Sd6-54Bvh6=s64","userId":"06359001126903823254"}},"outputId":"90d5d89b-ec0c-4817-886e-5b98566621fb"},"source":["import os\n","os.chdir('nubia')\n","!pip install -r requirements.txt\n","from nubia import Nubia\n","nubia = Nubia()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.6.0+cu101)\n","Collecting fairseq\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n","\u001b[K     |████████████████████████████████| 307kB 8.7MB/s \n","\u001b[?25hCollecting pytorch-transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n","\u001b[K     |████████████████████████████████| 184kB 20.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.5)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.16.0)\n","Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (0.22.2.post1)\n","Collecting wget\n","  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (0.16.0)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq->-r requirements.txt (line 2)) (1.14.2)\n","Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq->-r requirements.txt (line 2)) (0.29.21)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq->-r requirements.txt (line 2)) (2019.12.20)\n","Collecting sacrebleu\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/d3/be980ad7cda7c4bbfa97ee3de062fb3014fc1a34d6dd5b82d7b92f8d6522/sacrebleu-1.4.13-py3-none-any.whl (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq->-r requirements.txt (line 2)) (4.41.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers->-r requirements.txt (line 3)) (1.14.48)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 11.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers->-r requirements.txt (line 3)) (2.23.0)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 46.9MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.22.2->-r requirements.txt (line 6)) (1.4.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq->-r requirements.txt (line 2)) (2.20)\n","Collecting portalocker\n","  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers->-r requirements.txt (line 3)) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers->-r requirements.txt (line 3)) (0.10.0)\n","Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers->-r requirements.txt (line 3)) (1.17.48)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers->-r requirements.txt (line 3)) (7.1.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers->-r requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers->-r requirements.txt (line 3)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers->-r requirements.txt (line 3)) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers->-r requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->pytorch-transformers->-r requirements.txt (line 3)) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->pytorch-transformers->-r requirements.txt (line 3)) (0.15.2)\n","Building wheels for collected packages: fairseq, wget, sacremoses\n","  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2046421 sha256=2ce0798859f9651bc71f8283fad5edf74c7274e0e5f4a7fda06ef2be93a907b2\n","  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=b2dd08b3a4125964fa40f5f4b0b92e1d48cea47906b93201a03f246bb54bd5e7\n","  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=fb2b890adce8ed7f271dbadc9abddaa9ede41e3a3815cb4b1f34efc1fdf3f1ec\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built fairseq wget sacremoses\n","Installing collected packages: portalocker, sacrebleu, fairseq, sacremoses, sentencepiece, pytorch-transformers, wget\n","Successfully installed fairseq-0.9.0 portalocker-2.0.0 pytorch-transformers-1.2.0 sacrebleu-1.4.13 sacremoses-0.0.43 sentencepiece-0.1.91 wget-3.2\n","loading archive file pretrained/roBERTa_STS\n","| [input] dictionary: 50265 types\n"],"name":"stdout"},{"output_type":"stream","text":["1042301B [00:00, 1093367.25B/s]\n","456318B [00:00, 612016.45B/s]\n"],"name":"stderr"},{"output_type":"stream","text":["loading archive file pretrained/roBERTa_MNLI\n","| dictionary: 50264 types\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1042301/1042301 [00:00<00:00, 2022600.34B/s]\n","100%|██████████| 456318/456318 [00:00<00:00, 1322369.24B/s]\n","100%|██████████| 665/665 [00:00<00:00, 435542.19B/s]\n","100%|██████████| 548118077/548118077 [00:18<00:00, 29297558.80B/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"q1Cm4PZJk2B2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599046134207,"user_tz":180,"elapsed":221484,"user":{"displayName":"Cinthia Mikaela de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSYWVyWfXKPRe-m_hW9EaymViE56Sd6-54Bvh6=s64","userId":"06359001126903823254"}},"outputId":"170f83b7-bc59-4abe-dd72-bfe2f18c5590"},"source":["cd '/content/drive/My Drive/Colab Notebooks/SCIM/30_resultados/'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/SCIM/30_resultados\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pQ0JWlcP4mzT","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599066957714,"user_tz":180,"elapsed":7405123,"user":{"displayName":"Cinthia Mikaela de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSYWVyWfXKPRe-m_hW9EaymViE56Sd6-54Bvh6=s64","userId":"06359001126903823254"}},"outputId":"85e0d8ee-7428-4341-b2e6-ee91319ac5ce"},"source":["from bs4 import BeautifulSoup\n","from xml.etree import ElementTree\n","from xml.dom import minidom\n","from functools import reduce\n","from xml.etree.ElementTree import Element, SubElement, Comment\n","import numpy as np\n","\n","path = '/content/drive/My Drive/Colab Notebooks/SCIM/30_resultados/'\n","number_files = 30\n","\n","\n","rouge_1_arr  = []\n","rouge_2_arr  = []\n","rouge_L_arr  = []\n","bleurt_arr = []\n","NUBIA_arr = []\n","\n","def prettify(elem):\n","      \"\"\"Return a pretty-printed XML string for the Element.\n","      \"\"\"\n","      rough_string = ElementTree.tostring(elem, 'utf-8')\n","      reparsed = minidom.parseString(rough_string)\n","      return reparsed.toprettyxml(indent=\"  \")\n","  \n","\n","top = Element('ZakSum')\n","\n","comment = Comment('Generated by Amr Zaki')\n","top.append(comment)\n"," \n","i = 28\n","infile = open( path + \"result_model_\"+ str(i) +\".xml\" ,\"r\")\n","contents = infile.read()\n","soup = BeautifulSoup(contents, 'xml')\n","\n","print(\"Model: {}\".format(i))\n","\n","\n","for r in soup.find_all('example'):\n","\n","    article = r.find('article').get_text()\n","    reference = r.find('reference').get_text()\n","    candidate = r.find('summary').get_text()\n","\n","    example = SubElement(top, 'example')\n","    article_element   = SubElement(example, 'article')\n","    article_element.text = article\n","  \n","    reference_element = SubElement(example, 'reference')\n","    reference_element.text = reference\n","  \n","    summary_element   = SubElement(example, 'candidate')\n","    summary_element.text = candidate\n","\n","    rouge_1 = float((str(r.find('ROUGE_1')).replace(\"<ROUGE_1 score=\\\"\",\"\").replace(\"\\\"/>\",\"\")))\n","    rouge_2 = float((str(r.find('ROUGE_2')).replace(\"<ROUGE_2 score=\\\"\",\"\").replace(\"\\\"/>\",\"\")))\n","    rouge_l = float((str(r.find('ROUGE_l')).replace(\"<ROUGE_l score=\\\"\",\"\").replace(\"\\\"/>\",\"\")))\n","\n","    if(candidate != \"\"):\n","      nubia_score =  nubia.score(reference, candidate)\n","    else:\n","      nubia_score = 0\n","\n","    eval_element = SubElement(example, 'eval')\n","    ROUGE_1_element  = SubElement(eval_element, 'ROUGE_1' , {'score':str(rouge_1)})\n","    ROUGE_2_element  = SubElement(eval_element, 'ROUGE_2' , {'score':str(rouge_2)})\n","    ROUGE_L_element  = SubElement(eval_element, 'ROUGE_l' , {'score':str(rouge_l)})\n","    NUBIA_element =  SubElement(eval_element,'NUBIA', {'score':str(nubia_score)})\n","\n","    rouge_1_arr.append(rouge_1) \n","    rouge_2_arr.append(rouge_2) \n","    rouge_L_arr.append(rouge_l)\n","    NUBIA_arr.append(nubia_score)\n","\n","top.set('rouge_1', str(np.mean(rouge_1_arr)))\n","top.set('rouge_2', str(np.mean(rouge_2_arr)))\n","top.set('rouge_L', str(np.mean(rouge_L_arr)))\n","top.set('NUBIA', str(np.mean(NUBIA_arr)))\n","\n","with open(\"/content/drive/My Drive/Colab Notebooks/SCIM/NUBIA/model_\" + str (i) +\".xml\", \"w+\") as f:\n","  print(prettify(top), file=f)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Model: 28\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eRHqmc16l--g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599066957717,"user_tz":180,"elapsed":32,"user":{"displayName":"Cinthia Mikaela de Souza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSYWVyWfXKPRe-m_hW9EaymViE56Sd6-54Bvh6=s64","userId":"06359001126903823254"}},"outputId":"988f901d-c6a5-4630-d048-97cf91ae58f6"},"source":["#Sending mensage to slack\n","import re\n","import requests\n","import json\n","\n","web_hook_url = 'https://hooks.slack.com/services/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'\n","slack_msg = {'text': 'Validation was finished'}\n","requests.post(web_hook_url,data = json.dumps(slack_msg))"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Response [200]>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"HF3-wKk3Iqhi","colab_type":"text"},"source":["# **REFERENCES**\n","\n","\n","KANE, Hassan et al. NUBIA: NeUral Based Interchangeability Assessor for Text Generation. arXiv preprint arXiv:2004.14667, 2020.\n","\n","LIN, Chin-Yew. Rouge: A package for automatic evaluation of summaries. In: Text summarization branches out. 2004. p. 74-81.\n","\n","SELLAM, Thibault; DAS, Dipanjan; PARIKH, Ankur P. BLEURT: Learning Robust Metrics for Text Generation. arXiv preprint arXiv:2004.04696, 2020."]}]}